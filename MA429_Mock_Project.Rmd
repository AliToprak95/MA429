---
title: "MA429 MOCK PROJECT"
output: MA429_html_notebook
---

```{r}
#Download the data from UCI Machine Learning Repository
test <- read.table("https://archive.ics.uci.edu/ml/machine-learning-databases/adult/adult.test", skip=1, 
                   sep=",", header = FALSE,
                   col.names=c("Age", "Workclass", "Fnlwgt","Education","Education-num","Marital-status",
                               "Occupation","Relationship","Race","Sex",
                               "Capital-gain","Capital-loss","Hours-per-week","Native-country","Income-level"), na.strings = " ?")
train <- read.table("https://archive.ics.uci.edu/ml/machine-learning-databases/adult/adult.data",
                    sep=",", header = FALSE,
                    col.names=c("Age", "Workclass", "Fnlwgt","Education","Education-num","Marital-status",
                                "Occupation","Relationship","Race","Sex",
                                "Capital-gain","Capital-loss","Hours-per-week","Native-country","Income-level"), na.strings = " ?")


```


```{r}
#Number of missing variables for each attribute
missingnumber <- function(col){sum(is.na(col))}
missings <- sapply(train, missingnumber)
missings


levels(train$Income.level) <- c(0,1)
levels(test$Income.level) <- c(0,1)
#Structure of train data
str(train)
#Summary of train data
summary(train)
#Number of observations in training data has missing values
length(which(rowSums(is.na(train))>0))

```

```{r}
library(ggplot2)
theme_set(theme_classic())

#Plotting for Factor Variables
Workclassplot <- ggplot(train, aes(x=Workclass, fill=Income.level)) + geom_bar(stat = "count") + coord_flip() + theme(text = element_text(size=20)) 
Educationplot <- ggplot(train, aes(x=Education, fill=Income.level)) + geom_bar(stat = "count") + coord_flip() + theme(text = element_text(size=20)) 
Martial.statusplot <- ggplot(train, aes(x=Martial.status, fill=Income.level)) + geom_bar(stat = "count") + coord_flip() + theme(text = element_text(size=20)) 
Occupationplot <- ggplot(train, aes(x=Occupation, fill=Income.level)) + geom_bar(stat = "count") + coord_flip() + theme(text = element_text(size=20)) 
Relationshipplot <- ggplot(train, aes(x=Relationship, fill=Income.level)) + geom_bar(stat = "count") + coord_flip() + theme(text = element_text(size=20)) 
Raceplot <- ggplot(train, aes(x=Race, fill=Income.level)) + geom_bar(stat = "count") + coord_flip() + theme(text = element_text(size=20)) 
Sexplot <- ggplot(train, aes(x=Sex, fill=Income.level)) + geom_bar(stat = "count") + coord_flip() + theme(text = element_text(size=20)) 
Native.countryplot <- ggplot(train, aes(x=Native.country, fill=Income.level)) + geom_bar(stat = "count") + coord_flip() + theme(text = element_text(size=20)) 






Ageplot <- ggplot(train, aes(x=Age,fill=Income.level)) + geom_histogram(stat = "count") + theme(text = element_text(size=20))
library(ggpubr)
#figure <- ggarrange(Raceplot,Ageplot,
 #                   labels = c("Race", "Age"),
  #                  ncol = 2)

```


```{r}
library(purrr)
library(tidyr)
library(ggplot2)

train %>%
  keep(is.numeric) %>% 
  gather() %>% 
  ggplot(aes(value)) +
  facet_wrap(~ key, scales = "free") +
  geom_histogram()

sapply(train, class)
```


```{r}
#Age
attach(censusdata)
hist(Age,breaks=seq(min(Age),max(Age),by=1))
boxplot(Age)
install.packages("outliers")
library(outliers)
chisq.out.test(Age,variance = var(Age),opposite = TRUE)
chisq.out.test(Age,variance = var(Age),opposite = FALSE)
qqnorm(Age, main = "Normal QQ Plot - ylarge")
qqline(Age, col = "red")
```

```{r}
outlier_values <- boxplot.stats(Age)$out
boxplot(Age, main="Age", boxwex=0.1)
mtext(paste("Outliers: ", paste(outlier_values, collapse=", ")), cex=0.6)
qnt <- quantile(Age, probs=c(.25, .75), na.rm = T)
caps <- quantile(Age, probs=c(.05, .95), na.rm = T)
H <- 1.5 * IQR(Age, na.rm = T)
#censusdata[, censusdata$Age < (qnt[1] - H)]$Age <- caps[1]
#censusdata[, censusdata$Age > (qnt[2] + H)]$Age <- caps[2]
censusdata[which(censusdata$Age < (qnt[1] - H)),1] <- caps[1]
censusdata[which(censusdata$Age > (qnt[2] + H)),1] <- caps[2]
boxplot(censusdata$Age)
```
# Find outliers by boxplot
```{r}
numericdata <- censusdata[,sapply(census_complete, is.numeric)]
outlier_boxplot <- function(col){
outlier_values <- boxplot.stats(col)$out
length(outlier_values)
}
sapply(numericdata, outlier_boxplot)
```
# Find outliers by quantile
```{r}
outlier_qnt <- function(col){
qnt <- quantile(col, probs=c(.25, .75), na.rm = T)
caps <- quantile(col, probs=c(.05, .95), na.rm = T)
H <- 1.5 * IQR(col, na.rm = T)
length(which(col<(qnt[1] - H))) + length(which(col>(qnt[2] + H)))
}
sapply(numericdata,outlier_qnt)
```

# Plot the density of numeric variables with respect to the income level
```{r}
library(sm)
plotdata <- census_complete[,1:14]
income.f <- factor(census_complete$Income.level, labels = c("<= 50K", "> 50K"))
Plot <- function(col){
  sm.density.compare(col,group=census_complete$Income.level)
  colfill <- c(2:(2+length(levels(income.f))))
  legend("topright", levels(income.f), fill=colfill)
}
sapply(numericdata, Plot)
```

```{r}
library(scorecard)
bins = woebin(train, y="Income.level", positive = "1" )
bins
```

#Prepare the final dataset
```{r}
train_woe <-woebin_ply(dt = train, bins = bins)
test_woe <-woebin_ply(dt = test, bins = bins)

```
#Correlation between the variables
```{r}
library(corrplot)

correlations <- cor(train_woe[,c(-1,-13)])
correlationplot <- corrplot(correlations,type = "upper")
correlations

```

#Logistic Regression
```{r}

library(ROCR)

lr <- glm(Income.level ~ ., data = train_woe[,-13], family = binomial())
#Variation Inflation Factor
vif(lr)
summary(lr)
steplr <- step(lr,direction = 'both')
lr_predict <- predict(steplr,test_woe,type = "response")
lr_prediction <- prediction(lr_predict, test_woe$Income.level)
lr_confusion_matrix <- table(True_value = test_woe$Income.level, Predict_value = as.numeric(lr_predict > 0.5))
lr_confusion_matrix
lrperf <- performance(lr_prediction,measure = "tpr", x.measure = "fpr")
lrperfdata <- data.frame(FP = lrperf@x.values[[1]], TP = lrperf@y.values[[1]])


```




# random forest
```{r}
library(randomForest)
rf <- randomForest(Income.level~., data = train_woe)
rf
rf_predictprob <- predict(rf,test_woe,type = "prob")[,2]
rf_prediction <- prediction(rf_predictprob, test_woe$Income.level)
rf_predict <- predict(rf,test_woe)
rf_confusion_matrix <- table(True_value = test_woe$Income.level, Predict_value = rf_predict)
rf_confusion_matrix
rfperf <- performance(rf_prediction,measure = "tpr", x.measure = "fpr")
rfperfdata <- data.frame(FP = rfperf@x.values[[1]], TP = rfperf@y.values[[1]])
```
<<<<<<< HEAD

#xgboosting
```{r}
library(tidyverse)
library(caret)
library(xgboost)
library(doParallel)
trctrl <- trainControl(method = "cv", number = 5, allowParallel = FALSE)
xgb <- train(Income.level ~., data = train_woe, method = "xgbTree",
                trControl=trctrl)
xgbfinal<-xgb$finalModel
xgb_predictprob <- predict(xgbfinal,as.matrix(test_woe[,-1]))
xgb_predictprob <- 1 - xgb_predictprob
xgb_prediction <- prediction(xgb_predictprob, test_woe$Income.level)
xgb_confusion_matrix <- table(True_value = test_woe$Income.level, Predict_value = as.numeric(xgb_predictprob > 0.5))
xgb_confusion_matrix
xgbperf <- performance(xgb_prediction,measure = "tpr", x.measure = "fpr")
xgbperfdata <- data.frame(FP = xgbperf@x.values[[1]], TP = xgbperf@y.values[[1]])


```


=======
# Decision Tree
```{r}
library(tree)
tree <- tree(Income.level~.,train_woe)
summary(tree)
plot(tree)
text(tree, pretty = 0)
tree
tree_prediction <- predict(tree,test_woe, type = "class")
tree_cm <- table(test_woe$Income.level, tree_prediction)
tree_cm

```
# pruning the tree
```{r}
cv_tree <- cv.tree(tree, FUN = prune.misclass)
par(mfrow = c(1,2))
plot(cv_tree$size, cv_tree$dev, type = 'b')
plot(cv_tree$k, cv_tree$dev, type = 'b')
bestsize <- cv_tree$size[order(cv_tree$dev,decreasing = FALSE)][1]
message(paste0("best size found to be ", bestsize))
pruned_tree <- prune.misclass(tree, best = bestsize)
plot(pruned_tree)
text(pruned_tree, pretty = 0)

pruned_tree_prediction <- predict(pruned_tree, test_woe, type = "class")
pruned_tree_cm <- table(test_woe$Income.level,pruned_tree_prediction)
pruned_tree_cm
```
>>>>>>> 08a921bbc9c8a96c1724733f31c41604bf3193e4
# Performance Matrics
```{r}
getMetrics <- function(true_classes, predicted_class){
  confusion_matrix <- table(true_classes,predicted_class)
  true_neg <- confusion_matrix["0","0"]
  true_pos <- confusion_matrix["1","1"]
  false_neg <- confusion_matrix["1","0"]
  false_pos <- confusion_matrix["0", "1"]
  misclassification_rate <- mean(predicted_class != true_classes)
  precision <- true_pos / (true_pos + false_pos)
  recall <- true_pos / (true_pos + false_neg)
  return( list("Confusion Matrix" = confusion_matrix,
            "misclassification" = misclassification_rate, 
            "precision" = precision, 
            "recall" = recall))
}
```

#ROC CURVE PLOTTING
```{r}
theme_set(theme_grey())
g <- ggplot() + 
  geom_line(data = rfperfdata, aes(x = FP, y = TP, color = 'Random Forest')) + 
  geom_line(data = lrperfdata, aes(x = FP, y = TP, color = 'Logistic Regression')) + 
  geom_line(data = xgbperfdata, aes(x = FP, y = TP, color = 'XGBoost')) + 
  geom_segment(aes(x = 0, xend = 1, y = 0, yend = 1)) +
  ggtitle('ROC Curve') + 
  labs(x = 'False Positive Rate', y = 'True Positive Rate')
g
```

Add a new chunk by clicking the *Insert Chunk* button on the toolbar or by pressing *Ctrl+Alt+I*.

When you save the notebook, an HTML file containing the code and output will be saved alongside it (click the *Preview* button or press *Ctrl+Shift+K* to preview the HTML file).

The preview shows you a rendered HTML copy of the contents of the editor. Consequently, unlike *Knit*, *Preview* does not run any R code chunks. Instead, the output of the chunk when it was last run in the editor is displayed.
